# DevOps/SRE Technical Challenge

A complete DevOps/SRE solution demonstrating cloud-native best practices on AWS, including Infrastructure as Code, CI/CD, and comprehensive observability.

## üöÄ Features

- **Application**: Python Flask app with built-in instrumentation (Metrics & Logs).
- **Infrastructure**: AWS EKS (Managed Kubernetes) provisioned via Terraform.
- **CI/CD**: GitHub Actions pipeline with AWS OIDC authentication.
- **Networking**: AWS Load Balancer Controller managing ALB Ingress (Public Access).
- **Scaling**: Cluster Autoscaler and Spot Instances (`t3.small`).
- **Observability**: Full PLG Stack (Prometheus, Loki, Grafana) for metrics and logs.
- **Storage**: EBS CSI Driver for persistent storage (used by Loki).

## üõ†Ô∏è Architecture

- **Compute**: EKS Cluster with Managed Node Groups (Spot Instances).
- **Ingress**: AWS Application Load Balancer (ALB).
- **Monitoring**:
  - **Prometheus**: Scrapes `/metrics` from the app.
  - **Loki**: Collects logs via Promtail.
  - **Grafana**: Visualizes health and performance.
- **Security**: IAM Roles for Service Accounts (IRSA) for least privilege.

## üìÇ Repository Structure

- `app/`: Source code for the Python Flask application.
- `terraform/`: Infrastructure as Code (IaC) modules and environment configs.
- `k8s/`: Helm charts and Kubernetes manifests.
- `.github/workflows/`: CI/CD pipeline definitions.

## üö¶ Getting Started

### Prerequisites
- AWS CLI & Terraform installed.
- GitHub Repository with Actions enabled.

### 1. Infrastructure Deployment
Navigate to the terraform directory and apply the configuration:
```bash
cd terraform/environments/dev
terraform init
terraform apply
```

### 2. CI/CD Setup
1. Create a GitHub Repository Secret: `AWS_ACCOUNT_ID`.
2. Push to `main` branch to trigger the deployment pipeline.

### 3. Accessing the Application
After deployment, get the ALB URL:
```bash
kubectl get ingress
```

### 4. Accessing Monitoring (Grafana)
Port-forward the Grafana service:
```bash
kubectl port-forward svc/kube-prometheus-stack-grafana -n monitoring 3000:80
```
- **URL**: http://localhost:3000
- **User**: `admin`
- **Password**: generated by Terraform (sensitive)

Retrieve it after `terraform apply`:
```bash
cd terraform/environments/dev
terraform output -raw grafana_admin_password
```

Production note: avoid managing passwords in Terraform state. Prefer AWS Secrets Manager/SSM + External Secrets Operator and configure Grafana to use an existing Kubernetes Secret.

## üß™ Chaos & Testing
The application includes a "Chaos" feature to demonstrate monitoring:
1. Open the application in your browser.
2. Click the **"Action"** button multiple times.
3. It has a **20% chance** of returning a `500 Error`.
4. Check **Grafana** to see the error spike and **Loki** for the stack trace.

## üßπ Cleanup / Destroy (Cost Savings)

If you‚Äôre done with the demo, tear down resources to avoid ongoing AWS charges (EKS control plane, EC2 nodes, Load Balancers, and EBS volumes).

### Option A: Remove workloads only (keep the cluster)

This is useful if you want to stop most runtime costs while keeping the cluster for later.

```bash
# App (installed by CI via Helm)
helm uninstall sre-portal -n default

# Monitoring stack (if installed manually via Helm)
helm uninstall kube-prometheus-stack -n monitoring
helm uninstall loki-stack -n monitoring
```

### Option B: Destroy everything (recommended)

This removes the entire dev environment created by Terraform.

```bash
cd terraform/environments/dev
terraform destroy
```

### Verify nothing is left behind

In AWS Console (or via CLI), double-check these are gone to fully stop charges:
- ALB + target groups (created by Ingress)
- EBS volumes (e.g., Loki PVC)
- EC2 instances / node groups
- EKS cluster

## üìÑ License

MIT License

## üë§ Author

**Aetush**

---

**Note**: This is a technical assessment project demonstrating DevOps/SRE capabilities including containerization, Kubernetes orchestration, infrastructure as code, CI/CD automation, and cloud-native monitoring practices.
